{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T13:57:30.332666Z","iopub.status.busy":"2024-06-13T13:57:30.332259Z","iopub.status.idle":"2024-06-13T13:57:31.863061Z","shell.execute_reply":"2024-06-13T13:57:31.862022Z","shell.execute_reply.started":"2024-06-13T13:57:30.332629Z"},"trusted":true},"outputs":[],"source":["## Import packages here\n","import pandas as pd\n","import numpy as np \n","import imblearn\n","from matplotlib.pyplot import figure\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n","from sklearn.model_selection import train_test_split, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix, precision_score, recall_score, roc_auc_score\n","from sklearn.metrics import confusion_matrix, roc_curve, classification_report\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn import metrics\n","from sklearn.inspection import permutation_importance\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import GridSearchCV\n","from collections import Counter"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T13:57:35.453194Z","iopub.status.busy":"2024-06-13T13:57:35.452683Z","iopub.status.idle":"2024-06-13T13:57:35.462323Z","shell.execute_reply":"2024-06-13T13:57:35.461109Z","shell.execute_reply.started":"2024-06-13T13:57:35.453163Z"},"trusted":true},"outputs":[],"source":["# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T13:57:35.721615Z","iopub.status.busy":"2024-06-13T13:57:35.721210Z","iopub.status.idle":"2024-06-13T13:58:31.306377Z","shell.execute_reply":"2024-06-13T13:58:31.304807Z","shell.execute_reply.started":"2024-06-13T13:57:35.721584Z"},"trusted":true},"outputs":[],"source":["# # Installing packages\n","# !pip install duckdb\n","# !pip install watermark\n","# !pip install pysmiles\n","# !pip install rdkit"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T13:58:31.309516Z","iopub.status.busy":"2024-06-13T13:58:31.309007Z","iopub.status.idle":"2024-06-13T14:01:58.994530Z","shell.execute_reply":"2024-06-13T14:01:58.993140Z","shell.execute_reply.started":"2024-06-13T13:58:31.309454Z"},"trusted":true},"outputs":[],"source":["# # Database\n","# import duckdb\n","# # Database - Parquet format\n","# data_train = '/kaggle/input/leash-BELKA/train.parquet'\n","# test_path = '/kaggle/input/leash-BELKA/test.parquet'\n","\n","# # Bank connection\n","# con = duckdb.connect()\n","\n","# # Query\n","# train = con.query(f\"\"\"(SELECT * FROM parquet_scan('{data_train}') \n","# WHERE binds = 0\n","# ORDER BY random()\n","# LIMIT 5000000)\n","# UNION ALL\n","# (SELECT * FROM parquet_scan('{data_train}')\n","# WHERE binds = 1\n","# ORDER BY random()\n","# LIMIT 500000)\"\"\").df()\n","\n","# # Closing database\n","# con.close()\n","\n","# # Saving dataset\n","# train.to_csv(\"/kaggle/working/dataset.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:01:58.996781Z","iopub.status.busy":"2024-06-13T14:01:58.996341Z","iopub.status.idle":"2024-06-13T14:02:05.647567Z","shell.execute_reply":"2024-06-13T14:02:05.646328Z","shell.execute_reply.started":"2024-06-13T14:01:58.996742Z"},"trusted":true},"outputs":[],"source":["# # Viewing first 5 data\n","# #train_ = pd.read_csv('/kaggle/input/leash-BELKA/train.csv')\n","# test = pd.read_csv('/kaggle/input/leash-BELKA/test.csv')\n","# train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:06.314804Z","iopub.status.busy":"2024-06-13T14:02:06.314345Z","iopub.status.idle":"2024-06-13T14:02:06.326344Z","shell.execute_reply":"2024-06-13T14:02:06.324976Z","shell.execute_reply.started":"2024-06-13T14:02:06.314771Z"},"trusted":true},"outputs":[],"source":["#check the numbers of samples and features\n","# print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\n","# print(\"The test data size before dropping Id feature is : {} \".format(test.shape))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:06.330459Z","iopub.status.busy":"2024-06-13T14:02:06.330053Z","iopub.status.idle":"2024-06-13T14:02:15.596509Z","shell.execute_reply":"2024-06-13T14:02:15.595186Z","shell.execute_reply.started":"2024-06-13T14:02:06.330428Z"},"trusted":true},"outputs":[],"source":["# # Shuffle the rows\n","# train = train.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:15.598253Z","iopub.status.busy":"2024-06-13T14:02:15.597913Z","iopub.status.idle":"2024-06-13T14:02:17.446285Z","shell.execute_reply":"2024-06-13T14:02:17.445258Z","shell.execute_reply.started":"2024-06-13T14:02:15.598225Z"},"trusted":true},"outputs":[],"source":["# train_copy=train.copy()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:17.447984Z","iopub.status.busy":"2024-06-13T14:02:17.447610Z","iopub.status.idle":"2024-06-13T14:02:17.760233Z","shell.execute_reply":"2024-06-13T14:02:17.759082Z","shell.execute_reply.started":"2024-06-13T14:02:17.447954Z"},"trusted":true},"outputs":[],"source":["# Visualize the count for each class\n","# train_copy['binds'].value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:17.761930Z","iopub.status.busy":"2024-06-13T14:02:17.761581Z","iopub.status.idle":"2024-06-13T14:02:26.136312Z","shell.execute_reply":"2024-06-13T14:02:26.135164Z","shell.execute_reply.started":"2024-06-13T14:02:17.761902Z"},"trusted":true},"outputs":[],"source":["# train_copy.isnull().sum()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:26.137961Z","iopub.status.busy":"2024-06-13T14:02:26.137632Z","iopub.status.idle":"2024-06-13T14:02:26.538762Z","shell.execute_reply":"2024-06-13T14:02:26.537361Z","shell.execute_reply.started":"2024-06-13T14:02:26.137927Z"},"trusted":true},"outputs":[],"source":["# test.isnull().sum()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:26.540713Z","iopub.status.busy":"2024-06-13T14:02:26.540368Z","iopub.status.idle":"2024-06-13T14:02:26.547846Z","shell.execute_reply":"2024-06-13T14:02:26.546742Z","shell.execute_reply.started":"2024-06-13T14:02:26.540685Z"},"trusted":true},"outputs":[],"source":["# Get a Pd.Series consisting of all the string categoricals\n","#one_hot_encode_cols = data_copy.dtypes[data_copy.dtypes == 'object']  # filtering by string categoricals\n","#one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n","\n","#data_copy[one_hot_encode_cols].head().T"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:26.550908Z","iopub.status.busy":"2024-06-13T14:02:26.549220Z","iopub.status.idle":"2024-06-13T14:02:26.561187Z","shell.execute_reply":"2024-06-13T14:02:26.560088Z","shell.execute_reply.started":"2024-06-13T14:02:26.550873Z"},"trusted":true},"outputs":[],"source":["# Do the one hot encoding\n","#data_copy = pd.get_dummies(data_copy, columns=one_hot_encode_cols, drop_first=True)\n","#data_copy.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:26.564891Z","iopub.status.busy":"2024-06-13T14:02:26.564457Z","iopub.status.idle":"2024-06-13T14:02:36.620230Z","shell.execute_reply":"2024-06-13T14:02:36.618797Z","shell.execute_reply.started":"2024-06-13T14:02:26.564861Z"},"trusted":true},"outputs":[],"source":["#join the test and trainset to ensure uniform preprocessing\n","# ntrain = train_copy.shape[0]\n","# ntest = test.shape[0]\n","# y = train_copy[['binds']]\n","# all_data = pd.concat((train_copy, test)).reset_index(drop=True)\n","# all_data.drop(['binds'], axis=1, inplace=True)\n","# print(\"all_data size is : {}\".format(all_data.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from sklearn.preprocessing import LabelEncoder\n","\n","# # Create a LabelEncoder instance\n","# le = LabelEncoder()\n","\n","# # Fit the encoder to the entire dataset (flattened)\n","# le.fit(all_data[['buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'protein_name', 'molecule_smiles']].values.reshape(-1, 1))\n","\n","# # Transform each column using the same encoder\n","# all_data['buildingblock1_smiles'] = le.transform(all_data['buildingblock1_smiles'])\n","# all_data['buildingblock2_smiles'] = le.transform(all_data['buildingblock2_smiles'])\n","# all_data['buildingblock3_smiles'] = le.transform(all_data['buildingblock3_smiles'])\n","# all_data['protein_name'] = le.transform(all_data['protein_name'])\n","# all_data['molecule_smiles'] = le.transform(all_data['molecule_smiles'])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:36.622120Z","iopub.status.busy":"2024-06-13T14:02:36.621761Z","iopub.status.idle":"2024-06-13T14:02:36.635964Z","shell.execute_reply":"2024-06-13T14:02:36.634577Z","shell.execute_reply.started":"2024-06-13T14:02:36.622090Z"},"trusted":true},"outputs":[],"source":["# all_data.head()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:36.638165Z","iopub.status.busy":"2024-06-13T14:02:36.637804Z","iopub.status.idle":"2024-06-13T14:02:36.764517Z","shell.execute_reply":"2024-06-13T14:02:36.763145Z","shell.execute_reply.started":"2024-06-13T14:02:36.638137Z"},"trusted":true},"outputs":[],"source":["# import seaborn as sns\n","#all_data.drop('Unnamed: 0', axis=1, inplace= True)\n","#sns.pairplot(data_copy, plot_kws=dict(alpha=.1, edgecolor='none'))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T14:02:36.766750Z","iopub.status.busy":"2024-06-13T14:02:36.766243Z"},"trusted":true},"outputs":[],"source":["# import pandas as pd\n","# from rdkit import Chem\n","# from rdkit.Chem import Descriptors\n","# from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n","# from sklearn.preprocessing import StandardScaler\n","\n","# # Calculate molecular descriptors for building blocks and molecule\n","# def calculate_descriptors(smiles):\n","#     mol = Chem.MolFromSmiles(smiles)\n","#     descriptors = []\n","#     descriptors.append(Descriptors.MolWt(mol))\n","#     #descriptors.append(Descriptors.LogP(mol))\n","#     descriptors.append(Descriptors.TPSA(mol))\n","#     descriptors.append(Descriptors.NumHDonors(mol))\n","#     descriptors.append(Descriptors.NumHAcceptors(mol))\n","#     return descriptors\n","\n","# all_data['buildingblock1_descriptors'] = all_data['buildingblock1_smiles'].apply(lambda x: calculate_descriptors(x))\n","# all_data['buildingblock2_descriptors'] = all_data['buildingblock2_smiles'].apply(lambda x: calculate_descriptors(x))\n","# all_data['buildingblock3_descriptors'] = all_data['buildingblock3_smiles'].apply(lambda x: calculate_descriptors(x))\n","# all_data['molecule_descriptors'] = all_data['molecule_smiles'].apply(lambda x: calculate_descriptors(x))\n","# # Generate molecular fingerprints for building blocks and molecule\n","# def generate_fingerprints(smiles):\n","#     mol = Chem.MolFromSmiles(smiles)\n","#     fp = GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n","#     return fp\n","\n","# all_data['buildingblock1_fingerprints'] = all_data['buildingblock1_smiles'].apply(lambda x: generate_fingerprints(x))\n","# all_data['buildingblock2_fingerprints'] = all_data['buildingblock2_smiles'].apply(lambda x: generate_fingerprints(x))\n","# all_data['buildingblock3_fingerprints'] = all_data['buildingblock3_smiles'].apply(lambda x: generate_fingerprints(x))\n","# all_data['molecule_fingerprints'] = all_data['molecule_smiles'].apply(lambda x: generate_fingerprints(x))\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# # One-hot encode protein names\n","# protein_names = all_data['protein_name'].unique()\n","# all_data['protein_name'] = df['protein_name'].apply(lambda x: protein_names.tolist().index(x))\n","\n","# # Scale the features using StandardScaler\n","# scaler = StandardScaler()\n","# all_data['scaled_buildingblock1_descriptors'] = scaler.fit_transform(all_data['buildingblock1_descriptors'])\n","# all_data['scaled_buildingblock2_descriptors'] = scaler.fit_transform(all_data['buildingblock2_descriptors'])\n","# all_data['scaled_buildingblock3_descriptors'] = scaler.fit_transform(all_data['buildingblock3_descriptors'])\n","# all_data['scaled_molecule_descriptors'] = scaler.fit_transform(all_data['molecule_descriptors'])\n","# all_data['scaled_buildingblock1_fingerprints'] = scaler.fit_transform(all_data['buildingblock1_fingerprints'])\n","# all_data['scaled_buildingblock2_fingerprints'] = scaler.fit_transform(all_data['buildingblock2_fingerprints'])\n","# all_data['scaled_buildingblock3_fingerprints'] = scaler.fit_transform(all_data['buildingblock3_fingerprints'])\n","# all_data['scaled_molecule_fingerprints'] = scaler.fit_transform(all_data['molecule_fingerprints'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # One-hot encode protein names\n","# protein_names = all_data['protein_name'].unique()\n","# all_data['protein_name'] = df['protein_name'].apply(lambda x: protein_names.tolist().index(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(18,18))\n","# sns.heatmap(all_data.corr(),annot=True,cmap='RdYlGn')\n","# plt.show()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["new_df = pd.read_csv('feature_engineered_fingerprints.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["new_df = new_df[['id', 'buildingblock1_smiles',\n","       'buildingblock2_smiles', 'buildingblock3_smiles', 'molecule_smiles',\n","       'protein_name', 'buildingblock1_descriptors',\n","       'buildingblock2_descriptors', 'buildingblock3_descriptors',\n","       'molecule_descriptors']]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# split the Name column into two columns\n","new_df[['Molecular_Weight_block1', \n","    'Topological_Polar_Surface_Area_block1',\n","    'Number_of_Rotatable_Bonds_block1','Number_of_Hydrogen_Bond_Donors_block1']] = new_df['buildingblock1_descriptors'].str.split(',', expand=True)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# split the Name column into two columns\n","new_df[['Molecular_Weight_block2', \n","    'Topological_Polar_Surface_Area_block2',\n","    'Number_of_Rotatable_Bonds_block2','Number_of_Hydrogen_Bond_Donors_block2']] = new_df['buildingblock2_descriptors'].str.split(',', expand=True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# split the Name column into two columns\n","new_df[['Molecular_Weight_block3', \n","    'Topological_Polar_Surface_Area_block3',\n","    'Number_of_Rotatable_Bonds_block3','Number_of_Hydrogen_Bond_Donors_block3']] = new_df['buildingblock3_descriptors'].str.split(',', expand=True)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"ename":"MemoryError","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m new_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMolecular_Weight_molecule\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopological_Polar_Surface_Area_molecule\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber_of_Rotatable_Bonds_molecule\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber_of_Hydrogen_Bond_Donors_molecule\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmolecule_descriptors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\accessor.py:136\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     )\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\accessor.py:915\u001b[0m, in \u001b[0;36mStringMethods.split\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_re(pat):\n\u001b[0;32m    914\u001b[0m     regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(result, returns_string\u001b[38;5;241m=\u001b[39mexpand, expand\u001b[38;5;241m=\u001b[39mexpand)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\object_array.py:359\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_split\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    357\u001b[0m             n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    358\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(pat, n)\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\object_array.py:78\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     76\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_convert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     p_err \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m((takes)|(missing)) (?(2)from \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ to )?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?(3)required )positional arguments?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m     )\n","File \u001b[1;32mlib.pyx:2875\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\object_array.py:358\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_split.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    357\u001b[0m             n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 358\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(pat, n)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_str_map(f, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n","\u001b[1;31mMemoryError\u001b[0m: "]}],"source":["new_df[['Molecular_Weight_molecule', \n","    'Topological_Polar_Surface_Area_molecule',\n","    'Number_of_Rotatable_Bonds_molecule','Number_of_Hydrogen_Bond_Donors_molecule']] = new_df['molecule_descriptors'].str.split(',', expand=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_df.drop(['buildingblock3_descriptors', 'buildingblock2_descriptors', 'buildingblock1_descriptors'], axis= 1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_ = all_data[:ntrain]\n","test_ = all_data[ntrain:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rs=123\n","# Grid search hyperparameters for a random forest model\n","def grid_search_rf(X_train, y_train):\n","    params_grid = { \n","        'max_depth': [5, 10, 15, 20], \n","        'n_estimators': [25, 50, 100], \n","    }\n","    rf_model = RandomForestClassifier(random_state=rs)\n","    grid_search = GridSearchCV(estimator = rf_model, param_grid = params_grid, scoring='f1', cv = 2, verbose = 1)\n","    grid_search.fit(X_train.drop(['id'], axis=1), y_train)\n","    preds = preds = grid_search.predict(x_test.drop(['id'], axis=1))\n","    probs = grid_search.predict_proba(x_test.drop(['id'], axis=1))[:, 1]\n","    submit= grid_search.predict_proba(test_.drop(['id'], axis=1))[:, 1]\n","    submit_df = pd.DataFrame({'id': test_['id'], 'binds': submit})\n","    submit_df.to_csv('submit_1_rf.csv')\n","\n","    \n","    # Confusion matrix\n","    cm = confusion_matrix(y_test, preds)\n","    print('Confusion Matrix - Random Forest:\\n', cm)\n","    \n","    # ROC curve\n","    fpr, tpr, _ = roc_curve(y_test, probs)\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, label='Random Forest')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.title('ROC Curve - Random Forest')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.legend()\n","    plt.show()\n","    \n","    # Classification metrics report\n","    f1 = f1_score(y_test, preds)\n","    recall = recall_score(y_test, preds)\n","    precision = precision_score(y_test, preds)\n","    accuracy = accuracy_score(y_test, preds)\n","    print('Metrics - Random Forest:')\n","    print('F1 Score:', f1)\n","    print('Recall:', recall)\n","    print('Precision:', precision)\n","    print('Accuracy:', accuracy)\n","\n","    # Feature importance\n","    feature_importances = grid_search.best_estimator_.feature_importances_\n","    feature_names = X_train.drop(['id'], axis=1).columns\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(feature_names, feature_importances)\n","    plt.title('Feature Importances - Random Forest')\n","    plt.xlabel('Feature')\n","    plt.ylabel('Importance')\n","    plt.xticks(rotation=90)\n","    plt.show()\n","    \n","    \n","\n","# Grid search hyperparameters for a XGBoost model\n","def grid_search_xgb(X_train, y_train):\n","    params_grid = {\n","        'max_depth': [5, 10, 15, 20],\n","        'n_estimators': [25, 50, 100],\n","        'learning_rate': [0.1, 0.5, 1],\n","    }\n","    xgb_model = XGBClassifier(random_state=rs)\n","    grid_search = GridSearchCV(estimator=xgb_model, param_grid=params_grid, scoring='f1', cv=2, verbose=1)\n","    grid_search.fit(X_train.drop('id', axis=1), y_train)\n","    preds = grid_search.predict(x_test.drop('id', axis=1))\n","    probs = grid_search.predict_proba(x_test.drop(['id'], axis=1))[:, 1]\n","    submit= grid_search.predict_proba(test_.drop(['id'], axis=1))[:, 1]\n","    submit_df = pd.DataFrame({'id': test_['id'], 'binds': submit})\n","    submit_df.to_csv('submit_1__xg.csv', index=False)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_test, preds)\n","    print('Confusion Matrix - XGBoost:\\n', cm)\n","\n","    # ROC curve\n","    fpr, tpr, _ = roc_curve(y_test, probs)\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, label='XGBoost')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.title('ROC Curve - XGBoost')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.legend()\n","    plt.show()\n","\n","    # Feature importance\n","    feature_importances = grid_search.best_estimator_.feature_importances_\n","    feature_names = X_train.drop('id', axis=1).columns\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(feature_names, feature_importances)\n","    plt.title('Feature Importances - XGBoost')\n","    plt.xlabel('Feature')\n","    plt.ylabel('Importance')\n","    plt.xticks(rotation=90)\n","    plt.show()\n","\n","     # Classification metrics report\n","    f1 = f1_score(y_test, preds)\n","    recall = recall_score(y_test, preds)\n","    precision = precision_score(y_test, preds)\n","    accuracy = accuracy_score(y_test, preds)\n","    print('Metrics - XGBoost:')\n","    print('F1 Score:', f1)\n","    print('Recall:', recall)\n","    print('Precision:', precision)\n","    print('Accuracy:', accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def split_data(df, target):\n","    return train_test_split(df, target, test_size=0.3, stratify=y, random_state = rs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Split the training and testing dataset\n","x_train, x_test, y_train, y_test = split_data(train_, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["grid_search_rf(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier\n","grid_search_xgb(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.read_csv('submit_1__xg.csv').shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8006601,"sourceId":67356,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
